## 概述



**副本集是一组 MongoDB 的进程去维持同样的一份数据集，通过 MongoDB 的复制协议保证主备之间的数据一致性。**



MongoDB通过一张特殊的表 `local.oplog.rs`存储oplog，该表的特点是：固定大小，满了就会删除最旧的记录，插入新的记录，而且只支持append操作，因此可以理解为一个持久化的ring-buffer。opLog是MongoDB复制集的核心功能点



**MongoDB复制集是指，MongoDB实例通过复制并应用其他实例的oplog来达到数据冗余的技术**



MongoDB的复制集技术并不少见，很类似Mysql中的异步复制模式，只是MongoDB中多了对实例的心跳和master选举机制



一个完整的MongoDB复制集，包含了如下几点功能：

- 数据同步
  - initial-sync
  - steady-sync
  - 异常数据回滚
- MongoDB集群心跳与选举



![image-20210412145330957](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412145330957.png)



## MongoDB Oplog

MongoDB Oplog是MongoDB Primary和Secondary在复制建立期间和建立完成之后的复制介质，就是Primary中所有的写入操作都会记录到MongoDB Oplog中，然后从库会来主库一直拉取Oplog并应用到自己的数据库中。这里的Oplog是MongoDB local数据库的一个集合，它是Capped collection，通俗意思就是它是固定大小，循环使用的。如下图：

![image-20210412152756105](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412152756105.png)

MongoDB Oplog中的内容及字段介绍：

```json
{
"ts" : Timestamp(1446011584, 2),
"h" : NumberLong("1687359108795812092"),
"v" : 2,
"op" : "i",
"ns" : "test.nosql",
"o" : { "_id" : ObjectId("563062c0b085733f34ab4129"), "name" : "mongodb", "score" : "100" }
}

ts： 操作时间，当前timestamp + 计数器，计数器每秒都被重置
h：操作的全局唯一标识
v：oplog版本信息
op：操作类型
    i：插入操作
    u：更新操作
    d：删除操作
    c：执行命令（如createDatabase，dropDatabase）
n：空操作，特殊用途
ns：操作针对的集合
o：操作内容，如果是更新操作
o2：操作查询条件，仅update操作包含该字段
```

MongoDB目前已经迭代了很多个版本，下图汇总了目前市面上常用版本中MongoDB在复制的一些重要改进。

![image-20210412152923120](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412152923120.png)



### **oplog幂等性**

Oplog有一个非常重要的特性——幂等性（idempotent）。即对一个数据集合，使用oplog中记录的操作重放时，无论被重放多少次，其结果会是一样的。举例来说，如果oplog中记录的是一个插入操作，并不会因为你重放了两次，数据库中就得到两条相同的记录。



### oplog保留策略

在4.4之前

- 根据 `Replication.OplogSizeMB` 的配置值决定 Oplog 集合的大小上限，默认为 磁盘空间的 5%，如果用户是单机多实例的部署形态，需要调整默认值；
- 当 Oplog 集合大小超过上限时，会自动删除最老的 Oplog Entry。

4.4及之后

- MongoDB 提供了按时间段来保留 Oplog，由参数 `Storage. OplogMinRetentionHours` 控制，方便用户更好地完成定期维护的操作。
- 删除时，即使 Oplog 集合大小超过了配置的最⼤值，但最老的 Oplog 仍然在 `Storage.OplogMinRetentionHours` 范围内，那么 Oplog 不会删。

其默认大小设置如下：

- 在64位Linux、Windows操作系统上为当前分区可用空间的5%，但最大不会超过50G。

- 在64位的OS X系统中，MongoDB默认分片183M大小给Oplog。

- 在32位的系统中，MongoDB分片48MB的空间给Oplog。

  

### 复制时间窗口

使用**rs.printReplicationInfo()**可以查看oplog大小以及预计窗口覆盖时间。

复制时间窗口，意思就是说 oplog 中，可以保留大概多长时间的内容。因为oplog是一个封顶集合，当存储大小达到上限时，新写入的数据会覆盖最旧的数据。若从节点在一定时间内都没有进行复制操作，其源节点的oplog已经有部分覆盖了，这时从节点的oplog就是脏数据了。

举例：若果Oplog是大小是可用空间的5%，且可以存储24小时内的操作，那么从节点就可以在停止复制24小时后仍能追赶上主节点，而不需要重新获取全部数据。如果说从节点在24小时后开始追赶数据，那么不好意思主节点的oplog已经滚动覆盖了，把从节点没有执行的那条语句给覆盖了。这个时候为了保证数据一致性就会终止复制。此时从节点需要进行全量复制。



当然，若复制集中的操作没有这么频繁，oplog可以存放的时间则可以是一个比较大的数字了



### oplog大小设置

oplog的大小应该随着实际使用压力来增加大小，当集群中的主要操作是读操作，而写操作较少，oplog可以设置为默认大小甚至更小一点也可以，但当有以下情况时，应该是要设立更大的oplog

- 同时更新大量的文档。

  ​	Oplog为了保证 幂等性 会将多项更新（multi-updates）转换为一条条单条的操作记录。这就会在数据没有那么多变动的情况下大量的占用oplog空间。

- 删除了与插入时相同大小的数据

  ​	如果我们删除了与我们插入时同样多的数据，数据库将不会在硬盘使用情况上有显著提升，但是oplog的增长情况会显著提升。

- 大量In-Place更新

  ​	如果我们会有大量的in-place更新，数据库会记录下大量的操作记录，但此时硬盘中数据量不会有所变化。



## 节点分类

在复制集中，MongoDB按功能可分为以下节点

| 节点名               | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| 主节点(Primary)      | 可以提供读Writes/Read的节点，整个复制集中只有一个主节点。    |
| 隐藏节点(Hidden)     | 提供Read并对程序不可见的节点。                               |
| 延时节点(Delayed)    | 提供Read并能够延时复制的节点。                               |
| “投票”节点(Priority) | 提供Read并具有投票权的节点。                                 |
| 选举节点(Arbiter)    | Arbiter节点，无数据，仅作选举和充当复制集成员。又成为投票节点。 |



## 全量复制



数据同步分为好几个阶段：initial-sync，steady-sync，以及异常时的数据回滚



当一个节点刚加入集群时，**它需要初始化数据使得自身与集群中其他节点的数据量差距尽量减少**，这个过程称为initial-sync（全量复制）



一个initial-sync包含以下（源码位于 `rs_initialSync.cpp:_initialSync`函数的逻辑）

- 选取同步源
- 删除本地除local库以外的所有db
- 拉取主库存量数据（注意，此处只导入数据，不导入索引）
- 将在过程3中源产生的oplog同步到本地
- 从源中所有table的索引在本地重建
- 完成全量复制，源和本地的差距足够小，MongoDB进入Secondary（从节点）状态



### 同步源节点选取



这里先说明一点，MongoDB默认是采取级联复制的架构，就是默认不一定选择主库作为自己的同步源，如果不想让其进行级联复制，可以通过chainingAllowed参数来进行控制。在级联复制的情况下，你也可以通过`replSetSyncFrom`命令来指定你想复制的同步源。



MongoDB从库会在副本集其他节点通过以下条件筛选符合自己的同步源。

- 如果设置了chainingAllowed 为false，那么只能选取主库为同步源
- 找到与自己ping时间最小的并且数据比自己新的节点 （在副本集初始化的时候，或者新节点加入副本集的时候，新节点对副本集的其他节点至少ping两次）
- 该同步源与主库最新optime做对比，如果延迟主库超过30s，则不选择该同步源。
- 在第一次的过滤中，首先会淘汰比自己数据还旧的节点。如果第一次没有，那么第二次需要算上这些节点，防止最后没有节点可以做为同步源了。
- 最后确认该节点是否被禁止参与选举，如果是则跳过该节点。

通过上述筛选最后过滤出来的节点作为新的同步源。



注意，当选定同步源后，并不是一直稳定的，可能在以下情况下进行同步源变更：

- ping不通自己的同步源
- 自己的同步源发生了角色变化
- 自己的同步源与副本集任意一个节点延迟超过30s



### 拉取主库存量数据

这块步骤是全量复制的核心逻辑了，流程大概如下

![image-20210412160153576](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412160153576.png)

注意： 本图是针对于MongoDB 3.4之前的版本



同步流程大致如下

```txt
1.   Add _initialSyncFlag to minValid collection to tell us to restart initial sync if we crash in the middle of this procedure
2.   Record start time.（记录当前主库最近一次 oplog time）
3.   Clone.
4.   Set minValid1 to sync target’s latest op time.
5.   Apply ops from start to minValid1, fetching missing docs as needed.（Apply Oplog 1）
6.   Set minValid2 to sync target’s latest op time.
7.   Apply ops from minValid1 to minValid2.（Apply Oplog 2）
8.   Build indexes.
9.   Set minValid3 to sync target’s latest op time.
10.  Apply ops from minValid2 to minValid3.（Apply Oplog 3）
11.  Cleanup minValid collection: remove _initialSyncFlag field, set ts to minValid3 OpTime

// 翻译如下
1.   minValid集合设置_initialSyncFlag（db.replset.minvalid.find()）。
2.   获取同步源当前最新的oplog时间戳t0。
3.   从同步源克隆所有的集合数据。
4.   获取同步源最新的oplog时间戳t1。
5.   同步t0~t1所有的oplog。
6.   获取同步源最新的oplog时间戳t2。
7.   同步t1~t2所有的oplog。
8.   从同步源读取index信息，并建立索引（除了_id ，这个之前已经建立完成）。
9.   获取同步源最新的oplog时间戳t3。
10.  同步t2~t3所有的oplog。
11.  minValid集合清除_initialSyncFlag，initial sync结束。
```

以上为 网上博客的摘抄，下面结合翻译软件谈谈自己的理解

- _initialSyncFlag，标志位，存放于minValid集合，如果在同步过程中失败，则往 minValid 集合中写入标志位，表明全量复制失败，需要重新进行复制

- minValid1，minValid2，minValid3为三个阶段
  - 0 - minValid1，表示从源节点中同步oplog，这个阶段的同步时间较长，此时同步的oplog称为oplog1
  - minValid1 - minValid2，拉取并同步在阶段1中，源节点生成的oplog，此时同步的oplog称为oplog2
  - minValid2 - minValid3，拉取并同步在阶段2中，源节点生成的oplog，此时从节点与源节点的差距已经很小，在此节点生成从节点的索引信息

- 索引信息建立，minValid3阶段完成，删除 _initialSyncFlag标志，从节点完成全量复制，可以进入增量复制的阶段



![image-20210412175031541](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412175031541.png)







### 全量同步断点续传

![image-20210412181300015](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412181300015.png)



- 4.4之前，全量同步期间，如果发生网络异常，导致同步中断，需要重头开始，网络 环境比较差时，大数据量很难完成全量同步，可用节点数变少，实例可用性存在隐患；
- 4.4 : 基于「Resume Token」机制，记录全量拉取的位点，网络异常导致同步中断 后，重连时带上 Resume Token；
- `Replication.initialSyncTransientErrorRetryPeriodSeconds` 参数决定了同步中断 后重试的超时时间，默认 24h。



## 增量同步

![image-20210412182823805](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412182823805.png)



这里进行增量同步的不一定是 Primary，因为同步源也可能是Secondary，这里采用Primary主要是方便理解



共有六个步骤

- Sencondary 初始化同步完成之后，开始增量复制，通过produce线程在Primary oplog.rs集合上建立cursor，并且实时请求获取数据。（这个cursor可以理解为offset吧，表示同步的位移）

- Primary 返回oplog 数据给Secondary。

- Sencondary 读取到Primary 发送过来的oplog，将其写入到队列中。

- Sencondary 的同步线程会通过 `tryPopAndWaitForMore` 方法一直消费队列，当每次达到一定的条件之后，条件如下：

  - 总数据大于100M
  - 已经取到部分数据但没到100MB，但是目前队列没数据了，这个时候会阻塞等待一秒，如果还没有数据则本次取数据完成。

  上述两个条件满足一个之后，就会将数据给 prefetchOps 方法处理，prefetchOps方法主要将数据以database级别切分，便于后面多线程写入到数据库中。如果采用的WiredTiger引擎，那这里是以Docment ID 进行切分。

- 最终将划分好的数据以多线程的方式批量写入到数据库中（在从库批量写入数据的时候MongoDB会阻塞所有的读）。
- 然后再将Queue中的Oplog数据写入到Sencondary中的oplog.rs集合中。



在增量复制的过程中，会有两种情况发生

- 情况1：从节点失速，即源节点写入速度过快（或者相对的，从节点读取oplog的速度过慢），源节点的oplog覆盖了从节点用于同步的游标。

  ![情况1](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412205149640.png)

- 情况2：本节点之前是Primary，但是宕机了，重启后与新的Primary有不一致的oplog

![image-20210412205206351](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412205206351.png)



这两种情况在`bgsync.cpp:_produce`函数中，虽然这两种情况很不一样，但是最终都会进入 `bgsync.cpp:_rollback`函数处理，
对于第二种情况，处理过程在`rs_rollback.cpp`中，具体步骤为：

1. 维持本地与远程的两个反向游标，以线性的时间复杂度找到LCA（最近公共祖先，上conflict.png中为Record4）
   该过程与经典的两个有序链表找公共节点的过程类似，具体实现在roll_back_local_operations.cpp:syncRollBackLocalOperations中，读者可以自行思考这一过程如何以线性时间复杂度实现。
2. 针对本地（从节点）每个冲突的oplog，枚举该oplog的类型，推断出回滚该oplog需要的逆操作并记录（**目的是为了处理情况2中红色的冲突部分，然后再从primary中拉取新的数据**），如下：
   2.1: create_table -> drop_table
   2.2: drop_table -> 重新同步该表
   2.3: drop_index -> 重新同步并构建索引
   2.4: drop_db -> 放弃rollback，改由用户手工init_resync
   2.5: apply_ops -> 针对apply_ops 中的每一条子oplog，递归执行 2)这一过程
   2.6: create_index -> drop_index
   2.7: 普通文档的CUD操作 -> 从Primary重新读取真实值并替换。相关函数为：`rs_rollback.cpp:refetch`
3. 针对2）中分析出的每条oplog的处理方式，执行处理，相关函数为 rs_rollback.cpp:syncFixUp，此处操作主要是对步骤2）的实践，实际处理过程相当繁琐。
4. truncate掉本地冲突的oplog。



对于本地失速的情况，也是走_rollback流程统一处理的，对于失速，走 _rollback时会在找LCA这步失败，之后会尝试更换复制源，方法为：**从当前存活的所有secondary和primary节点中找一个使自己“不处于失速”的节点。**

对于失速的判断，只有一个唯一的标准：**本地维护在复制源的游标被复制源的写覆盖**



### 同步线程模型与指令乱序加速



增量同步过程的线程模型，其过程脱不开就是一个 producer-consumer 模型。由于oplog需要保证顺序性，producer只能是单线程实现。

而对于consumer端，并发提速有如下方法

1. 首先，不相干的文档之间无需保证oplog apply的顺序，因此可以对oplog 按照objid 哈希分组。每一组内必须保证严格的写入顺序性。

2. 其次对于command命令，会对表或者库有全局性的影响，因此command命令必须在当前的consumer完成工作之后单独处理，而且在处理command oplog时，不能有其他命令同时执行。

3. 从库和主库的oplog 顺序必须完全一致，因此不管1、2步写入用户数据的顺序如何，oplog的必须保证顺序性。对于mmap引擎的capped-collection，只能以顺序插入来保证，因此对oplog的插入是单线程进行的。对于wiredtiger引擎的capped-collection，可以在ts(时间戳字段)上加上索引，从而保证读取的顺序与插入的顺序无关。

![image-20210412211554167](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412211554167.png)



## 高可用

MongoDB则是自己在内部已经实现了高可用方案。其实现方式就是基于raft协议实现了自身的高可用协议



### 触发场景

首先我们看那些情况会触发MongoDB执行主从切换。

- 新初始化一套副本集
- 从库不能连接到主库（默认超过10s，可通过heartbeatTimeoutSecs参数控制），从库发起选举
- 主库主动放弃primary 角色
  - 主动执行rs.stepdown 命令
  - 主库与大部分节点都无法通信的情况下
  - 修改副本集配置的时候（在Mongo 2.6版本会触发，其他版本待确定）
  - 修改以下配置的时候：
    - _id
    - votes
    - priotity
    - arbiterOnly
    - slaveDelay
    - hidden
    - buildIndexes
- 移除从库的时候（在MongoDB 2.6会触发，MongoDB 3.4不会，其他版本待确定）



### 心跳机制

集群内部是通过心跳机制来判断实例是否存活，当达到一定条件时，MongoDB主库或者从库就会触发切换。

![image-20210412212110765](https://raw.githubusercontent.com/VanniAmor/ImgBed/master/image-20210412212110765.png)



**心跳频率默认是2秒一次，也可以通过`heartbeatIntervalMillis`来进行控制**

在新节点加入进来的时候，副本集中所有的节点需要与新节点建立心跳，其内容为：

```c
BSONObjBuilder cmdBuilder;
cmdBuilder.append("replSetHeartbeat", setName);
cmdBuilder.append("v", myCfgVersion);
cmdBuilder.append("pv", 1);
cmdBuilder.append("checkEmpty", checkEmpty);
cmdBuilder.append("from", from);
if (me &gt; -1) {
cmdBuilder.append("fromId", me);
}  
```



具体在MongoDB日志中表现如下：

```
command admin.$cmd command: replSetHeartbeat { replSetHeartbeat: "shard1", v: 21, pv: 1, checkEmpty: false, from: "10.13.32.244:40011", fromId: 3 } ntoreturn:1 keyUpdates:0
```

那副本集所有节点默认都是每2秒给其他剩余的节点发送上述信息，在其他节点收到信息后会调用ReplSetCommand命令来处理心跳信息，处理完成会返回如下信息：

```cpp
result.append("set", theReplSet-&gt;name());
MemberState currentState = theReplSet-&gt;state();
result.append("state", currentState.s);  // 当前节点状态
if (currentState == MemberState::RS_PRIMARY) {
    result.appendDate("electionTime", theReplSet-&gt;getElectionTime().asDate());
}
result.append("e", theReplSet-&gt;iAmElectable());  //是否可以参与选举
result.append("hbmsg", theReplSet-&gt;hbmsg());
result.append("time", (long long) time(0));
result.appendDate("opTime", theReplSet-&gt;lastOpTimeWritten.asDate());
const Member *syncTarget = replset::BackgroundSync::get()-&gt;getSyncTarget();
if (syncTarget) {
    result.append("syncingTo", syncTarget-&gt;fullName());
}

int v = theReplSet-&gt;config().version;
result.append("v", v);
if( v &gt; cmdObj["v"].Int() )
    result &lt;&lt; &quot;config&quot; &lt;config().asBson();
```



### 切换流程

​	主节点选举是一个二阶段过程+多数派协议。

​	如果超出 ElectionTimeoutMillis（默认 10 秒）没有探测到主节点，Secondary 节点 会发起选举，发起前检查自身条件：

- Priority 是否大于 0
- 当前状态是否够新

在真正选举前，会先进行一轮空投（Dry-Run），避免当前 Primary 无意义的降级（ StepDown），因为 Primary 收到其他节点且 Term 更高的话则会降级；

Dry-Run 成功后，会增加自身的 Term 发起真正的选举，如果收到多数派的选票则选 举成功，把新的拓扑信息通过心跳广播到整个副本。



#### 阶段一

以自身POV，检测自身是否有被选举的资格：

1. 能ping通集群的过半数节点
2. priority必须大于0
3. 不能是arbitor节点
   如果检测通过，向集群中所有存活节点发送FreshnessCheck（询问其他节点关于“我”是否有被选举的资格）

同僚仲裁

选举第一阶段中，某节点收到其他节点的选举请求后，会执行更严格的同僚仲裁

1. 集群中有其他节点的primary比发起者高
2. 不能是arbitor节点
3. primary必须大于0
4. 以冲裁者的POV，发起者的oplog 必须是集群存活节点中oplog最新的（可以有相等的情况，大家都是最新的）

#### 第二阶段

发起者向集群中存活节点发送Elect请求，仲裁者收到请求的节点会执行一系列合法性检查，如果检查通过，则仲裁者给发起者投一票，并获得30秒钟“选举锁”，选举锁的作用是：在持有锁的时间内不得给其他发起者投票。

**发起者如果或者超过半数的投票，则选举通过，自身成为Primary节点**。获得低于半数选票的原因，除了常见的网络问题外，相同优先级的节点同时通过第一阶段的同僚仲裁并进入第二阶段也是一个原因。因此，当选票不足时，会sleep[0,1]秒内的随机时间，之后再次尝试选举。



## 回滚

主要是因为主库宕机，但是新写入的数据还没有来得及同步到从库中，这个时候就会发生数据丢失的情况。

那针对这种情况，MongoDB增加了回滚的机制。在主库恢复后重新加入到复制集中，这个时候老主库会与同步源对比oplog信息，这时候分为以下两种情况：

1. 在同步源中没有找到比老主库新的oplog信息。
2.  同步源最新一条oplog信息跟老主库的optime和oplog的hash内容不同。

回滚的过程就是逆向对比oplog的信息，直到在老主库和同步源中找到对应的oplog，然后将这期间的oplog全部记录到rollback目录里的文件中

如果但是出现以下情况会终止回滚：

- 对比老主库的optime和同步源的optime，如果超过了30分钟，那么放弃回滚。
- 在回滚的过程中，如果发现单条oplog超过512M，则放弃回滚。
- 如果有dropDatabase操作，则放弃回滚。
- 最终生成的回滚记录超过300M，也会放弃回滚。





## 参考文献

https://www.cnblogs.com/purpleraintear/p/6035111.html

https://taosha.club/topic/6055819783038c2e1dc16dc4#toc_14

https://www.yuanmas.com/info/GlypM6EoO2.html

https://mongoing.com/archives/5200